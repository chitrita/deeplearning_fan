{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Fan Zhang\n",
    "# 2018-10-16\n",
    "# Deep Variational Canonical Correlation (VCCA-private)\n",
    "# Modify Weiran Wang, 2017 code in single-cell and bulk\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import vcca_IM as vcca\n",
    "from myreadinput import read_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z': 10,\n",
       " 'H1': 10,\n",
       " 'H2': 10,\n",
       " 'IM': 0.0,\n",
       " 'dropprob': 0.0,\n",
       " 'checkpoint': './vcca_res'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using argparse and this module is used to write user-friendly command-line interfaces, \n",
    "# so it seems, it has a conflict with Jupyter Notebook.\n",
    "# import argparse\n",
    "# parser=argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--Z\", default=10, help=\"Dimensionality of features\", type=int)\n",
    "# parser.add_argument(\"--H1\", default=10, help=\"Dimensionality of private variables for view 1\", type=int) \n",
    "# parser.add_argument(\"--H2\", default=10, help=\"Dimensionality of private variables for view 2\", type=int)\n",
    "# parser.add_argument(\"--IM\", default=0.0, help=\"Regularization constant for the IM penalty\", type=float)\n",
    "# parser.add_argument(\"--dropprob\", default=0.0, help=\"Dropout probability of networks.\", type=float) \n",
    "# parser.add_argument(\"--checkpoint\", default=\"./vcca_test\", help=\"Path to saved models\", type=str) \n",
    "# args=parser.parse_args()\n",
    "from easydict import EasyDict as edict\n",
    "args = edict({\n",
    "    \"Z\": 10,\n",
    "    \"H1\": 10,\n",
    "    \"H2\": 10,\n",
    "    \"IM\": 0.0,\n",
    "    \"dropprob\": 0.0, # May try hier dropout: 0.3 with improved performance\n",
    "    \"checkpoint\": \"./vcca_res\"\n",
    "})\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Set random seeds.\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensionality of shared variables: 10\n",
      "Dimensionality of view 1 private variables: 10\n",
      "Dimensionality of view 2 private variables: 10\n",
      "Regularization constant for IM penalty: 0.000000\n",
      "Dropout rate: 0.000000\n",
      "Trained model will be saved at ./vcca_res\n"
     ]
    }
   ],
   "source": [
    "    # Obtain parsed arguments.\n",
    "    Z=args.Z\n",
    "    print(\"Dimensionality of shared variables: %d\" % Z)\n",
    "    H1=args.H1\n",
    "    print(\"Dimensionality of view 1 private variables: %d\" % H1)\n",
    "    H2=args.H2\n",
    "    print(\"Dimensionality of view 2 private variables: %d\" % H2)\n",
    "    IM_penalty=args.IM\n",
    "    print(\"Regularization constant for IM penalty: %f\" % IM_penalty)\n",
    "    dropprob=args.dropprob\n",
    "    print(\"Dropout rate: %f\" % dropprob)\n",
    "    checkpoint=args.checkpoint\n",
    "    print(\"Trained model will be saved at %s\" % checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Some other configurations parameters\n",
    "    losstype1=0\n",
    "    losstype2=1\n",
    "    learning_rate=0.0001\n",
    "    l2_penalty=0\n",
    "    latent_penalty=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Define network architectures.\n",
    "    # Replace 784 with the number of genes of inputs\n",
    "    network_architecture=dict(\n",
    "        # n_input1=784, \n",
    "        # n_input2=784, \n",
    "        n_input1=724, \n",
    "        n_input2=724, \n",
    "        n_z=Z,  # Dimensionality of shared latent space\n",
    "        # F_hidden_widths=[1024, 1024, 1024, Z],\n",
    "        F_hidden_widths=[1024, 1024, 1024, Z],\n",
    "        F_hidden_activations=[tf.nn.relu, tf.nn.relu, tf.nn.relu, None],\n",
    "        n_h1=H1, # Dimensionality of individual latent space of view 1\n",
    "        G1_hidden_widths=[1024, 1024, 1024, H1],\n",
    "        G1_hidden_activations=[tf.nn.relu, tf.nn.relu, tf.nn.relu, None],\n",
    "        n_h2=H2, # Dimensionality of individual latent space of view 2\n",
    "        G2_hidden_widths=[1024, 1024, 1024, H2],\n",
    "        G2_hidden_activations=[tf.nn.relu, tf.nn.relu, tf.nn.relu, None],\n",
    "        # H1_hidden_widths=[1024, 1024, 1024, 784],\n",
    "        H1_hidden_widths=[1024, 1024, 1024, 724],\n",
    "        H1_hidden_activations=[tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.sigmoid],\n",
    "        # H2_hidden_widths=[1024, 1024, 1024, 784],\n",
    "        H2_hidden_widths=[1024, 1024, 1024, 724],\n",
    "        H2_hidden_activations=[tf.nn.relu, tf.nn.relu, tf.nn.relu, tf.nn.sigmoid]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building view 1 recognition network F ...\n",
      "\tLayer 1 ...\n",
      "\tLayer 2 ...\n",
      "\tLayer 3 ...\n",
      "\tLayer 4 ...\n",
      "Building view 1 private network G1 ...\n",
      "\tLayer 1 ...\n",
      "\tLayer 2 ...\n",
      "\tLayer 3 ...\n",
      "\tLayer 4 ...\n",
      "Building view 2 private network G2 ...\n",
      "\tLayer 1 ...\n",
      "\tLayer 2 ...\n",
      "\tLayer 3 ...\n",
      "\tLayer 4 ...\n",
      "Building view 1 reconstruction network H1 ...\n",
      "\tLayer 1 ...\n",
      "\tLayer 2 ...\n",
      "\tLayer 3 ...\n",
      "\tLayer 4 ...\n",
      "Building view 2 reconstruction network H2 ...\n",
      "\tLayer 1 ...\n",
      "\tLayer 2 ...\n",
      "\tLayer 3 ...\n",
      "\tLayer 4 ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "    # First, build the model.\n",
    "    model=vcca.VCCA(network_architecture, losstype1, losstype2, learning_rate, l2_penalty, latent_penalty)\n",
    "    saver=tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint file not given or not existent!\n"
     ]
    }
   ],
   "source": [
    "    # Second, load the saved moded, if provided.\n",
    "    if checkpoint and os.path.isfile(checkpoint):\n",
    "        print(\"loading model from %s \" % checkpoint)\n",
    "        saver.restore(model.sess, checkpoint)\n",
    "        epoch=model.sess.run(model.epoch)\n",
    "        print(\"picking up from epoch %d \" % epoch)\n",
    "        tunecost=model.sess.run(model.tunecost)\n",
    "        print(\"tuning cost so far:\")\n",
    "        print(tunecost[0:epoch])\n",
    "    else:\n",
    "        print(\"checkpoint file not given or not existent!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-10-97465b486ccb>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-97465b486ccb>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "    # File for saving classification results.\n",
    "    classfile=checkpoint + '_classify.mat'\n",
    "    if os.path.isfile(classfile):\n",
    "        print(\"Job is already finished!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./sc_724rows_5265cols.csv.gz already downloaded\n",
      "Preprocessing dataset\n",
      "Finished preprocessing dataset\n",
      "File ./bulk_724rows_167cols.csv.gz already downloaded\n",
      "Preprocessing dataset\n",
      "Finished preprocessing dataset\n",
      "(5265, 724)\n",
      "(167, 724)\n"
     ]
    }
   ],
   "source": [
    "    # Third, load the data.\n",
    "    from scvi.dataset import LoomDataset, CsvDataset, Dataset10X, AnnDataset\n",
    "    data1_raw = CsvDataset(\"sc_724rows_5265cols.csv.gz\", save_path='./', new_n_genes=724, compression='gzip')\n",
    "    data2_raw = CsvDataset(\"bulk_724rows_167cols.csv.gz\", save_path='./', new_n_genes=724, compression='gzip')\n",
    "    print(np.shape(data1_raw.X))\n",
    "    print(np.shape(data2_raw.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5265, 41)\n",
      "(167, 87)\n"
     ]
    }
   ],
   "source": [
    "# Import meta data\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "sc_meta = pd.read_table(\"meta_sc_724rows_5265cols.tsv\")\n",
    "bulk_meta = pd.read_table(\"meta_bulk_724rows_167cols.tsv\")\n",
    "print(np.shape(sc_meta))\n",
    "print(np.shape(bulk_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5265, 724)\n",
      "(5265, 724)\n"
     ]
    }
   ],
   "source": [
    "# Extend the dataset with less samples to the same number of samples of the other dataset\n",
    "N = np.int64(np.floor(np.shape(data1_raw.X)[0]/np.shape(data2_raw.X)[0]))\n",
    "test = np.vstack([data2_raw.X]* N)\n",
    "# print(np.shape(data1_raw.X)[0] - np.shape(test)[0])\n",
    "b = data2_raw.X[:(np.shape(data1_raw.X)[0] - np.shape(test)[0]),:]\n",
    "test = np.concatenate((test, b), axis=0)\n",
    "print(np.shape(test))\n",
    "# Shuffle the rows\n",
    "data2_extend = np.take(test, np.random.permutation(test.shape[0]),axis=0,out=test);\n",
    "print(np.shape(data2_extend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3369, 41)\n",
      "(843, 41)\n",
      "(1053, 41)\n",
      "(3369, 724)\n",
      "(3369, 724)\n",
      "(1053, 724)\n",
      "(1053, 724)\n"
     ]
    }
   ],
   "source": [
    "def load_single_celldata(ratio_train, dataset, meta):\n",
    "#     print(np.shape(dataset))\n",
    "    df_matrix = np.array(dataset)\n",
    "#     df_matrix_t = df_matrix.transpose()\n",
    "#     print(np.shape(df_matrix_t))\n",
    "    N, D = df_matrix.shape\n",
    "    #Split the training data (training and validation) and test dataset.\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    ind = np.random.permutation(N)\n",
    "    \n",
    "    train_val_data = df_matrix[ind[:ind_cut], :]\n",
    "    test = df_matrix[ind[ind_cut:], :]\n",
    "    meta_train_val = np.array(meta)[ind[:ind_cut],:]\n",
    "    meta_test = np.array(meta)[ind[ind_cut:],:]\n",
    "    \n",
    "    #Split the training and validation dataset.\n",
    "    N, D = train_val_data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    ind = np.random.permutation(N)\n",
    "    \n",
    "    #Input data\n",
    "    train = train_val_data[ind[:ind_cut], :]\n",
    "    tune = train_val_data[ind[ind_cut:], :]\n",
    "    meta_train = meta_train_val[ind[:ind_cut], :]\n",
    "    meta_tune = meta_train_val[ind[ind_cut:], :]\n",
    "\n",
    "    return train, tune, test, meta_train, meta_tune, meta_test\n",
    "\n",
    "def load_bulkdata(ratio_train, dataset):\n",
    "#     print(np.shape(dataset))\n",
    "    df_matrix = np.array(dataset)\n",
    "#     df_matrix_t = df_matrix.transpose()\n",
    "#     print(np.shape(df_matrix_t))\n",
    "    N, D = df_matrix.shape\n",
    "    #Split the training data (training and validation) and test dataset.\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    ind = np.random.permutation(N)\n",
    "    \n",
    "    train_val_data = df_matrix[ind[:ind_cut], :]\n",
    "    test = df_matrix[ind[ind_cut:], :]\n",
    "    \n",
    "    #Split the training and validation dataset.\n",
    "    N, D = train_val_data.shape\n",
    "    ind_cut = int(ratio_train * N)\n",
    "    ind = np.random.permutation(N)\n",
    "    \n",
    "    #Input data\n",
    "    train = train_val_data[ind[:ind_cut], :]\n",
    "    tune = train_val_data[ind[ind_cut:], :]\n",
    "\n",
    "    return train, tune, test\n",
    "\n",
    "trainData1,tuneData1,testData1,meta_trainData1,meta_tuneData1,meta_testData1= load_single_celldata(ratio_train = 0.8, dataset = data1_raw.X, meta = sc_meta)\n",
    "trainData2,tuneData2,testData2= load_bulkdata(ratio_train = 0.8, dataset = data2_extend)\n",
    "\n",
    "# meta_trainData1[:10,1:10]\n",
    "print(np.shape(meta_trainData1))\n",
    "print(np.shape(meta_tuneData1))\n",
    "print(np.shape(meta_testData1))\n",
    "\n",
    "print(np.shape(trainData1))\n",
    "print(np.shape(trainData2))\n",
    "\n",
    "print(np.shape(testData1))\n",
    "print(np.shape(testData2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3369,)\n",
      "(843,)\n",
      "(1053,)\n"
     ]
    }
   ],
   "source": [
    "trainLabel = np.full(np.shape(trainData1)[0], \"0\")\n",
    "print(np.shape(trainLabel))\n",
    "\n",
    "tuneLabel = np.full(np.shape(tuneData1)[0], \"1\")\n",
    "print(np.shape(tuneLabel))\n",
    "\n",
    "testLabel = np.full(np.shape(testData1)[0], \"2\")\n",
    "print(np.shape(testLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3369\n",
      "3369\n",
      "3369\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainData1.shape[0])\n",
    "print(trainLabel.shape[0])\n",
    "print(trainData2.shape[0])\n",
    "trainData1.shape[0] == trainLabel.shape[0]\n",
    "trainData2.shape[0] == trainLabel.shape[0]\n",
    "\n",
    "tuneData1.shape[0] == tuneLabel.shape[0]\n",
    "tuneData2.shape[0] == tuneLabel.shape[0]\n",
    "\n",
    "testData1.shape[0] == testLabel.shape[0]\n",
    "testData2.shape[0] == testLabel.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type conversion view 1\n",
      "type conversion view 2\n",
      "type conversion view 1\n",
      "type conversion view 2\n",
      "type conversion view 1\n",
      "type conversion view 2\n"
     ]
    }
   ],
   "source": [
    "trainData,tuneData,testData=read_sc(trainData1,trainData2,trainLabel,tuneData1,tuneData2,tuneLabel,testData1,testData2,testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3369"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current learning rate 0.000100\n",
      "Epoch: 0001, train regret=-4859.82466442, tune cost=-10805.88281250\n",
      "latent_loss=196.47549438, nll1=-12943.04199219, nll2=1940.68383789\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0002, train regret=-11238.72505635, tune cost=-11222.63769531\n",
      "latent_loss=141.02593994, nll1=-13282.30468750, nll2=1918.64038086\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0003, train regret=-11520.87269034, tune cost=-11586.35839844\n",
      "latent_loss=98.11020660, nll1=-13549.32812500, nll2=1864.85913086\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0004, train regret=-11855.02445435, tune cost=-11750.96582031\n",
      "latent_loss=69.35379028, nll1=-13657.86621094, nll2=1837.54650879\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0005, train regret=-12012.25616721, tune cost=-11826.46679688\n",
      "latent_loss=62.54430389, nll1=-13697.68261719, nll2=1808.67175293\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0006, train regret=-11999.02705620, tune cost=-11882.30859375\n",
      "latent_loss=53.87675476, nll1=-13724.59960938, nll2=1788.41381836\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0007, train regret=-12122.92555863, tune cost=-11916.49023438\n",
      "latent_loss=51.68725586, nll1=-13750.17187500, nll2=1781.99426270\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0008, train regret=-12128.93925544, tune cost=-11942.43750000\n",
      "latent_loss=47.64667511, nll1=-13769.83789062, nll2=1779.75390625\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0009, train regret=-12149.51291184, tune cost=-11962.14746094\n",
      "latent_loss=44.25757217, nll1=-13784.25878906, nll2=1777.85327148\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0010, train regret=-12137.26697346, tune cost=-11982.75000000\n",
      "latent_loss=44.71419907, nll1=-13804.31933594, nll2=1776.85559082\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0011, train regret=-12185.47196057, tune cost=-12006.63085938\n",
      "latent_loss=43.86175537, nll1=-13825.22265625, nll2=1774.73046875\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0012, train regret=-12234.76344636, tune cost=-12023.80664062\n",
      "latent_loss=43.75326157, nll1=-13842.48730469, nll2=1774.92736816\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0013, train regret=-12236.48162822, tune cost=-12043.72265625\n",
      "latent_loss=42.71978760, nll1=-13858.39746094, nll2=1771.95507812\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0014, train regret=-12341.90511604, tune cost=-12078.99023438\n",
      "latent_loss=40.93255234, nll1=-13888.76171875, nll2=1768.83911133\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0015, train regret=-12245.44541569, tune cost=-12096.54687500\n",
      "latent_loss=38.76619339, nll1=-13903.25683594, nll2=1767.94335938\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0016, train regret=-12258.72961886, tune cost=-12104.14648438\n",
      "latent_loss=38.40834427, nll1=-13910.43652344, nll2=1767.88183594\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0017, train regret=-12414.66555821, tune cost=-12117.68945312\n",
      "latent_loss=39.14633560, nll1=-13921.56933594, nll2=1764.73315430\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0018, train regret=-12283.73381498, tune cost=-12127.16308594\n",
      "latent_loss=38.35036469, nll1=-13928.24414062, nll2=1762.73022461\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0019, train regret=-12365.93283198, tune cost=-12134.20117188\n",
      "latent_loss=38.76451874, nll1=-13934.30371094, nll2=1761.33801270\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0020, train regret=-12416.55090068, tune cost=-12143.62988281\n",
      "latent_loss=36.90636444, nll1=-13941.29296875, nll2=1760.75683594\n",
      "Model saved in file: ./vcca_res\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0021, train regret=-12365.80153027, tune cost=-12145.97753906\n",
      "latent_loss=38.08563232, nll1=-13942.63867188, nll2=1758.57495117\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0022, train regret=-12345.77615761, tune cost=-12153.15722656\n",
      "latent_loss=36.96434784, nll1=-13948.42480469, nll2=1758.30407715\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0023, train regret=-12463.52040665, tune cost=-12161.14550781\n",
      "latent_loss=37.07634354, nll1=-13954.61914062, nll2=1756.39709473\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0024, train regret=-12497.84951140, tune cost=-12169.06445312\n",
      "latent_loss=36.12545776, nll1=-13960.80175781, nll2=1755.61267090\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0025, train regret=-12433.27194990, tune cost=-12167.92968750\n",
      "latent_loss=36.38353348, nll1=-13959.71289062, nll2=1755.39978027\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0026, train regret=-12460.38342470, tune cost=-12178.56054688\n",
      "latent_loss=36.31341171, nll1=-13969.71386719, nll2=1754.84033203\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0027, train regret=-12432.57126210, tune cost=-12182.97851562\n",
      "latent_loss=37.27275467, nll1=-13972.68945312, nll2=1752.43798828\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0028, train regret=-12531.57511966, tune cost=-12188.55859375\n",
      "latent_loss=36.15845108, nll1=-13976.41699219, nll2=1751.70068359\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0029, train regret=-12464.57038323, tune cost=-12196.15820312\n",
      "latent_loss=36.76715851, nll1=-13984.08398438, nll2=1751.15856934\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0030, train regret=-12532.20686313, tune cost=-12199.44531250\n",
      "latent_loss=36.88026428, nll1=-13985.41015625, nll2=1749.08459473\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0031, train regret=-12524.05014934, tune cost=-12200.52441406\n",
      "latent_loss=36.41647720, nll1=-13985.48828125, nll2=1748.54809570\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0032, train regret=-12481.90950463, tune cost=-12208.26953125\n",
      "latent_loss=36.90405273, nll1=-13993.42675781, nll2=1748.25268555\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0033, train regret=-12514.69152220, tune cost=-12210.87597656\n",
      "latent_loss=36.65690231, nll1=-13995.23144531, nll2=1747.69812012\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0034, train regret=-12337.97124981, tune cost=-12212.74609375\n",
      "latent_loss=38.48108292, nll1=-13997.60351562, nll2=1746.37597656\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0035, train regret=-12662.64545998, tune cost=-12219.91015625\n",
      "latent_loss=38.91278076, nll1=-14001.81933594, nll2=1742.99609375\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0036, train regret=-12387.45344038, tune cost=-12218.46875000\n",
      "latent_loss=38.13554382, nll1=-14002.35058594, nll2=1745.74572754\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0037, train regret=-12742.73578144, tune cost=-12223.25000000\n",
      "latent_loss=37.88564301, nll1=-14006.54785156, nll2=1745.41198730\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0038, train regret=-12497.92115381, tune cost=-12232.11523438\n",
      "latent_loss=38.41680908, nll1=-14011.90820312, nll2=1741.37573242\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0039, train regret=-12568.78234106, tune cost=-12235.85351562\n",
      "latent_loss=38.53065109, nll1=-14014.47460938, nll2=1740.09033203\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0040, train regret=-12562.57646232, tune cost=-12241.98144531\n",
      "latent_loss=39.52761078, nll1=-14020.08007812, nll2=1738.57092285\n",
      "Model saved in file: ./vcca_res\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0041, train regret=-12787.15478443, tune cost=-12243.41894531\n",
      "latent_loss=39.46097183, nll1=-14020.49316406, nll2=1737.61364746\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0042, train regret=-12494.32774353, tune cost=-12253.94433594\n",
      "latent_loss=39.77536392, nll1=-14033.96093750, nll2=1740.24157715\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0043, train regret=-12727.97828895, tune cost=-12260.23339844\n",
      "latent_loss=39.16372299, nll1=-14034.55078125, nll2=1735.15344238\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0044, train regret=-12656.61690111, tune cost=-12255.07714844\n",
      "latent_loss=39.74827576, nll1=-14042.32421875, nll2=1747.49926758\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0045, train regret=-12605.75825078, tune cost=-12272.92578125\n",
      "latent_loss=41.28725433, nll1=-14051.73730469, nll2=1737.52416992\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0046, train regret=-12697.34572458, tune cost=-12289.64062500\n",
      "latent_loss=41.32329178, nll1=-14062.85351562, nll2=1731.88916016\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0047, train regret=-12768.13750023, tune cost=-12288.81250000\n",
      "latent_loss=41.51310349, nll1=-14062.26269531, nll2=1731.93701172\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0048, train regret=-12735.17128947, tune cost=-12296.15234375\n",
      "latent_loss=41.40589905, nll1=-14067.79296875, nll2=1730.23486328\n",
      "Current learning rate 0.000100\n",
      "Epoch: 0049, train regret=-12592.61617181, tune cost=-12297.42382812\n",
      "latent_loss=41.64516068, nll1=-14068.92480469, nll2=1729.85559082\n",
      "Current learning rate 0.000100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050, train regret=-12726.69746795, tune cost=-12306.11816406\n",
      "latent_loss=41.84127426, nll1=-14076.74218750, nll2=1728.78283691\n"
     ]
    }
   ],
   "source": [
    "# Traning.\n",
    "# batch_size = 64, 128, 256, 512\n",
    "# max_epochs = 50, 100, 150, 200\n",
    "# dropprob = 0.0, 0.1, 0.2, 0.3\n",
    "model=vcca.train(model, trainData, tuneData, saver, checkpoint, batch_size=52, max_epochs=50, save_interval=20, keepprob=(1.0-dropprob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_16:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot lower bound of objective function\n",
    "model.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize z\n",
    "print(\"Visualizing shared variables!\")\n",
    "trainData,tuneData,testData=read_sc(trainData1,trainData2,trainLabel,tuneData1,tuneData2,tuneLabel,testData1,testData2,testLabel)\n",
    "z_train, _=model.transform_shared(1, trainData.images1)\n",
    "z_tune, _=model.transform_shared(1, tuneData.images1)\n",
    "z_test, _=model.transform_shared(1, testData.images1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tSNE on the z_test\n",
    "from sklearn.manifold import TSNE\n",
    "tsne=TSNE(perplexity=20, n_components=2, init=\"pca\", n_iter=3000)\n",
    "z_tsne=tsne.fit_transform(np.asfarray(z_test[::2], dtype=\"float\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing private variables!\n"
     ]
    }
   ],
   "source": [
    "if H1>0:\n",
    "        print(\"Visualizing private variables!\")\n",
    "        h1_test, _= model.transform_private(1, testData.images1)\n",
    "        h1_train, _= model.transform_private(1, trainData.images1)\n",
    "        tsne=TSNE(perplexity=20, n_components=2, init=\"pca\", n_iter=3000)\n",
    "        h1_tsne=tsne.fit_transform( np.asfarray(h1_test[::2], dtype=\"float\") )\n",
    "else:\n",
    "        h1_test=-1\n",
    "        h1_tsne=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing private variables!\n"
     ]
    }
   ],
   "source": [
    "if H2>0:\n",
    "        print(\"Visualizing private variables!\")\n",
    "        h2_test, _= model.transform_private(1, testData.images2)\n",
    "        h2_train, _= model.transform_private(1, trainData.images2)\n",
    "        tsne=TSNE(perplexity=20, n_components=2, init=\"pca\", n_iter=3000)\n",
    "        h2_tsne=tsne.fit_transform( np.asfarray(h2_test[::2], dtype=\"float\") )\n",
    "else:\n",
    "        h2_test=-1\n",
    "        h2_tsne=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3369, 10)\n",
      "(527, 2)\n",
      "(843, 10)\n",
      "(1053, 10)\n",
      "(1053, 10)\n",
      "(527, 2)\n",
      "(1053, 10)\n",
      "(527, 2)\n",
      "(3369, 10)\n",
      "(3369, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(z_train))\n",
    "print(np.shape(z_tsne))\n",
    "print(np.shape(z_tune))\n",
    "print(np.shape(z_test))\n",
    "print(np.shape(h1_test))\n",
    "print(np.shape(h1_tsne))\n",
    "print(np.shape(h2_test))\n",
    "print(np.shape(h2_tsne))\n",
    "print(np.shape(h1_train))\n",
    "print(np.shape(h2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat(classfile, {\n",
    "        # 'tuneerr':best_error_tune,  'testerr':best_error_test,\n",
    "        'z_train':z_train,  'z_tune':z_tune,  'z_test':z_test,  'z_tsne':z_tsne,\n",
    "        'h1_test':h1_test,  'h1_tsne':h1_tsne,  'h2_test':h2_test,  'h2_tsne':h2_tsne,\n",
    "        'meta_trainData1':meta_trainData1[:,1],  'meta_tuneData1':meta_tuneData1[:,1],  'meta_testData1':meta_testData1[:,1]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(meta_trainData1))\n",
    "# meta_trainData1[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
